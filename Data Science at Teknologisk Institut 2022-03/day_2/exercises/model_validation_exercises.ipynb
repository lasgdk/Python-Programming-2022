{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation exercises\n",
    "These exercises are about tuning a model, and getting familiar with the way this is done. As quite a few steps are involved, this set of exercises, requires you to fill in values in places that are marked with `# FILL IN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform tuning, we need to be able to create a parameter grid. There is a built in function for this in scikit-learn, and this exercise is about understanding what it does. \n",
    "The `ParameterGrid` class takes a dictionary as argument. The dictionary should contain the names of the hyperparameters as keys, and a list containing the hyperparameter values. If you want to know more about dictionaries in python check this link https://www.tutorialspoint.com/python3/python_dictionary.htm.\n",
    "Let us see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to ParameterGrid class from sklearn\n",
    "grid_inp = {'param0': [5,6,7], 'param1': [10,100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this into the `ParameterGrid` class, we get an object containing all combinations for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class to create parameter grid\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Create a grid using the ParameterGrid class\n",
    "grid = ParameterGrid(grid_inp)\n",
    "\n",
    "# Print values\n",
    "print(\"Parameter combinations:\")\n",
    "for elem in grid:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the grid contains all combinations of `param0` and `param1`. An example of a valid value for `param0` or `param1` is `max_depth` if using the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises in model validation\n",
    "The first exercise is a bit long, since it involves the whole process of fitting and validating a model. Below you will find a code template with some bits and pieces missing. You are supposed to fill out the remaining lines. Everything that should be filled is indicated by a comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to load the data and create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('gaussian_2d_train.csv', index_col = 0)\n",
    "df_test = pd.read_csv('gaussian_2d_test.csv', index_col = 0)\n",
    "\n",
    "# sklearn has a built in tool to split data.\n",
    "# Here it is used to split the training set into a training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, vali = train_test_split(df_train, test_size = 0.30)\n",
    "\n",
    "# Split into features and labels\n",
    "X_train = train[['a','b']]\n",
    "y_train = train['group']\n",
    "X_vali = vali[['a','b']]\n",
    "y_vali = vali['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a grid of hyperparameters for tuning the model. Create a dictionary with values for `max_depth`. Don't use more than three values (So it doesn't take too long). Afterwards you can try adding `min_samples_split` to the parameter grid. If you are in doubt, see the example in the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import class to create parameter grid\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# ParameterGrid input\n",
    "param_grid = {} # FILL IN\n",
    "\n",
    "# Create a grid using the ParameterGrid class\n",
    "grid = ParameterGrid() # FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model, and create a `DataFrame` with two columns, one for the accuracy and one for the corresponding `max_depth`. The number of rows should be equal to the size of the parameter grid. Afterwards fill out the values needed for the loop to run through different hyperparameters and to fit and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "res = pd.DataFrame(np.zeros(( , )), columns = ['Acc', 'max_depth'])# FILL IN\n",
    "\n",
    "# Loop over the grid\n",
    "for row, elem in enumerate(grid):\n",
    "    \n",
    "    # Initialize model, set the correct max_depth\n",
    "    model = DecisionTreeClassifier(max_depth = )# FILL IN\n",
    "    \n",
    "    # Fit model on training data\n",
    "    model.fit(X = , y = )# FILL IN\n",
    "    \n",
    "    # Predict on validation data\n",
    "    pred = model.predict(X = )# FILL IN\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = # FILL IN\n",
    "    \n",
    "    # Store results\n",
    "    res.iloc[row, :] = [accuracy, elem['max_depth']] # For min_samples_split: elem['min_samples_split']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best value of `max_depth` in `res` by looking at which one yielded the best accuracy. Afterwards refit the model with the best hyperparameter on both the training and validation data, and predict on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters\n",
    "max_score = res['Acc'].max()\n",
    "best_rows = res.loc[res['Acc'] == max_score, :]\n",
    "best_max_d = # FILL IN\n",
    "\n",
    "# Retrain model with best param on full training data\n",
    "model = DecisionTreeClassifier(max_depth = best_max_d)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X = , y = )# FILL IN\n",
    "\n",
    "# Predict on test\n",
    "pred_test = model.predict(X = )# FILL IN\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy on test data is \" + )# FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('gaussian_2d_train.csv', index_col = 0)\n",
    "df_test = pd.read_csv('gaussian_2d_test.csv', index_col = 0)\n",
    "\n",
    "# sklearn has a built in tool to split data.\n",
    "# Here it is used to split the training set into a training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, vali = train_test_split(df_train, test_size = 0.30)\n",
    "\n",
    "# Split into features and labels\n",
    "X_train = train[['a','b']]\n",
    "y_train = train['group']\n",
    "X_vali = vali[['a','b']]\n",
    "y_vali = vali['group']\n",
    "\n",
    "# Import class to create parameter grid\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# ParameterGrid input\n",
    "param_grid = {'max_depth': [1,2,3]}\n",
    "\n",
    "# Create a grid using the ParameterGrid class\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "res = pd.DataFrame(np.zeros((len(grid), 2)), columns = ['Acc', 'max_depth'])\n",
    "\n",
    "# Loop over the grid\n",
    "for row, elem in enumerate(grid):\n",
    "    \n",
    "    print(elem)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DecisionTreeClassifier(max_depth = elem['max_depth'])\n",
    "    \n",
    "    # Fit model on training data\n",
    "    model.fit(X = X_train, y = y_train)\n",
    "    \n",
    "    # Predict on validation data\n",
    "    pred = model.predict(X = X_vali)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.sum(pred == y_vali)/pred.shape[0]\n",
    "    \n",
    "    # Store results\n",
    "    res.iloc[row, 0] = accuracy\n",
    "    res.iloc[row, 1] = elem['max_depth']\n",
    "\n",
    "print(res)\n",
    "\n",
    "# Get best hyperparameters\n",
    "max_score = res['Acc'].max()\n",
    "best_rows = res.loc[res['Acc'] == max_score, :]\n",
    "best_max_d = int(best_rows.iloc[0, 1])\n",
    "\n",
    "# Retrain model with best param on full training data\n",
    "model = DecisionTreeClassifier(max_depth = best_max_d)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X = df_train[['a','b']], y = df_train['group'])\n",
    "\n",
    "# Predict on test\n",
    "pred_test = model.predict(X = df_test[['a','b']])\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy on test data is \" + str(np.sum(pred_test == df_test['group'])/pred_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "The next exercise is to try out cross-validation. Use `GridSearchCV` from `sklearn` to perform K-fold cross-validation. The `GridSearchCV` object fits the model on the different subfolds automatically, and identifies the best set of hyperparameters. The best set is stored in the `best_estimator_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize model\n",
    "mod = DecisionTreeClassifier()\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('gaussian_2d_train.csv', index_col = 0)\n",
    "df_test = pd.read_csv('gaussian_2d_test.csv', index_col = 0)\n",
    "\n",
    "# Create a parameter grid\n",
    "param_grid = {} # FILL IN\n",
    "\n",
    "# Initialize grid search CV\n",
    "gs_cv = GridSearchCV(mod, param_grid, cv = 5)\n",
    "\n",
    "# Start cross validation with the fit method\n",
    "# FILL IN\n",
    "\n",
    "# Predict on the test set using the predict method\n",
    "pred = # FILL IN\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"Accuracy on test set: {0}\".format())# FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize model\n",
    "mod = DecisionTreeClassifier()\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('gaussian_2d_train.csv', index_col = 0)\n",
    "df_test = pd.read_csv('gaussian_2d_test.csv', index_col = 0)\n",
    "\n",
    "# Create a parameter grid\n",
    "param_grid = {'max_depth': [1,2,3]}\n",
    "\n",
    "# Initialize grid search CV\n",
    "gs_cv = GridSearchCV(mod, param_grid, cv = 5)\n",
    "\n",
    "# Start cross validation\n",
    "gs_cv.fit(X = df_train[['a','b']], y = df_train['group'])\n",
    "\n",
    "# Predict on the test set\n",
    "pred = gs_cv.predict(X = df_test[['a', 'b']])\n",
    "\n",
    "# Calculate accuracy\n",
    "print(\"Accuracy on test set: {0}\".format(np.sum(pred == df_test['group'])/pred.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom cross-validation\n",
    "The exercise is about creating your own cross-validation function. The function should take an initialized model, the training data as a pandas DataFrame, the name of the label column and a number `k` as argument where `k` indicates the number of folds to use. Calculate the loss as accuracy. You are free to do it in the way you see fit, but here is a way that it could be done:\n",
    "\n",
    "You could make use of the random number generator in numpy which is called `np.random.randint`. If you want to do 5 fold CV you could generate a random number from 0 to 4 for all rows in the data frame. Afterwards you could fit a model on all rows having 0, 1, 2, 3 and predict on those with index 4. Do the same where you change the hold out set to the other numbers and finally calculate the loss.\n",
    "\n",
    "Another hint: if you want to select all columns except one, you can do the following:\n",
    "```python\n",
    "df.loc[:, df.columns != EXC_COL]\n",
    "```\n",
    "where `EXC_COL` contains the name of the column to exclude. This is similar to the `drop` method which does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "def cv_itr(model, data, label_name, k):\n",
    "    # Find number of rows in data\n",
    "    rows = data.shape[0]\n",
    "    \n",
    "    # Generate random numbers for each row with vals between 0 and k.\n",
    "    idx = np.random.randint(0, k, rows)\n",
    "    \n",
    "    # Create a vector to store predictions\n",
    "    pred = np.zeros(rows)\n",
    "    \n",
    "    # Go through all folds\n",
    "    for i in range(k):\n",
    "        # Create dataset to fit on\n",
    "        data_fit = data.loc[idx != i, :]\n",
    "        \n",
    "        # Create a dataset to predict on\n",
    "        data_pred = data.iloc[idx == i, :]\n",
    "        \n",
    "        # Fit model\n",
    "        model.fit(X = data_fit.loc[:, data_fit.columns != label_name], y = data_fit[label_name])\n",
    "        \n",
    "        # Predict\n",
    "        pred[idx == i] = model.predict(X = data_pred.loc[:, data_pred.columns != label_name])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = np.sum(pred == data[label_name])/rows\n",
    "    \n",
    "    return acc\n",
    "\n",
    "cv_itr(DecisionTreeClassifier(), df_train, 'group', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the function `cv_itr` that you just made into a into a new function takes a model and a parameter grid. The function should change the hyperparameters of the model to a new value from the grid for each iteration. This can be done quite easily:\n",
    "```python\n",
    "model.set_params(max_depth = 2)\n",
    "```\n",
    "or if you are storing the hyperparameters in a dictionary:\n",
    "```python\n",
    "hyperparam = {\"max_depth\": 2}\n",
    "model.set_params(**hyperparam)\n",
    "```\n",
    "In the loop a call to `cv_itr` should be made in order to perform cross-validation on the model for the current hyperparameter set. The function should return the DataFrame with hyperparameters and corresponding scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def custom_cv_acc(model, data, label_name, k, param_grid):\n",
    "    # Create the actual grid\n",
    "    grid = ParameterGrid(param_grid)\n",
    "    \n",
    "    # Count the number of hyper-parameter combinations in the grid\n",
    "    n_runs = len(grid) # Or manually if you are cool: np.prod([len(val) for key, val in param_grid.items()])\n",
    "    \n",
    "    # Count number of hyperparameters\n",
    "    n_hyp_param = len(param_grid)\n",
    "    \n",
    "    # Get names of hyper params:\n",
    "    hyp_names = list(param_grid.keys())\n",
    "    \n",
    "    # Create dataframe to store hyper-param and score\n",
    "    res = pd.DataFrame(np.zeros((n_runs, n_hyp_param + 1)), columns = ['acc'] + hyp_names)\n",
    "    \n",
    "    # Loop through hyper parameters\n",
    "    for idx, g in enumerate(grid):\n",
    "        # Set new hyperparameters on the model\n",
    "        model.set_params(**g)\n",
    "        \n",
    "        # Use the function from last exercise to do the actual CV\n",
    "        acc = cv_itr(model, data, label_name, k)\n",
    "        \n",
    "        # Store result\n",
    "        res.iloc[idx, 0] = acc\n",
    "        \n",
    "        # Store hyper-param\n",
    "        res.iloc[idx, 1:] = [g[x] for x in hyp_names]\n",
    "    \n",
    "    return res\n",
    "\n",
    "# Run function\n",
    "custom_cv_acc(DecisionTreeClassifier(), df_train, 'group', 4 , param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning a model on your own\n",
    "This exercise is similar to the \"Fill-in\" exercise, except that now you have to code the whole process yourself. It is recommended, that you try to do this without looking too much on the exercise above. That way you can test how much you actually remember.\n",
    "\n",
    "In this exercise we will be using the breast cancer dataset, which can be found in the file `breast.csv`. The target variable is called `diagnosis`.\n",
    "\n",
    "You should go through the following steps:\n",
    "- Load data\n",
    "- Split into train and validation\n",
    "- Initialize model and create parameter grid\n",
    "- Loop over parameters while training on the train dataset and predicting on the validation dataset\n",
    "- Evaluate the performance of the model on the validation set, to find the best hyperparameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "br = pd.read_csv(\"breast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG\n",
    "# Hide code tagged with #ANS\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "function code_hide() {\n",
    "    var cells = IPython.notebook.get_cells()\n",
    "    cells.forEach(function(x){ if(x.get_text().includes(\"#ANS\")){\n",
    "        if (x.get_text().includes(\"#CONFIG\")){\n",
    "\n",
    "        } else{\n",
    "            x.input.hide()\n",
    "            x.output_area.clear_output()\n",
    "        }\n",
    "\n",
    "        \n",
    "    }\n",
    "    })\n",
    "}\n",
    "function code_hide2() {\n",
    "    var cells = IPython.notebook.get_cells();\n",
    "    cells.forEach(function(x){\n",
    "    if( x.cell_type != \"markdown\"){\n",
    "        x.input.show()      \n",
    "    }\n",
    "    \n",
    "        });\n",
    "} \n",
    "$( document ).ready(code_hide);\n",
    "$( document ).ready(code_hide2);\n",
    "</script>\n",
    "<form action=\"javascript:code_hide()\"><input type=\"submit\" value=\"Hide answers\"></form>\n",
    "<form action=\"javascript:code_hide2()\"><input type=\"submit\" value=\"Show answers\"></form>''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
