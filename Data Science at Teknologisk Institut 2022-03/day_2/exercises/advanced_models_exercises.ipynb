{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises in Advanced models\n",
    "We will look into Random Forest and Boosting models, and try to tune their hyperparameters. For these exercises we will be using the titanic dataset in order to predict survival of passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start out by executing the following cell which will load `titanic_train.csv` and turn it into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "train0 = pd.read_csv('../../day_1/exercises/titanic_train.csv')\n",
    "\n",
    "# Create labels\n",
    "y_train = train0['Survived']\n",
    "\n",
    "# Columns to perform one hot encoding on\n",
    "ohe_cols = ['Sex', 'Embarked']\n",
    "\n",
    "# String and index columns to drop\n",
    "drop_cols = ['Cabin', 'Ticket', 'Name', 'PassengerId', 'Survived']\n",
    "\n",
    "# Create OHE features\n",
    "train_ohe = pd.get_dummies(train0, columns = ohe_cols)\n",
    "\n",
    "# Drop string cols\n",
    "train_drop = train_ohe.drop(drop_cols, axis = 1)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer()\n",
    "imputed_vals = imputer.fit_transform(X = train_drop)\n",
    "X_train = pd.DataFrame(imputed_vals, columns = train_drop.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `RandomForestClassifier` from `sklearn.ensemble`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform a grid search cross validation using a built in module from `sklearn`. Import `GridSearchCV` from `sklearn.model_selection`, and define a parameter grid for the Random Forest model. We will be tuning the number of trees, `n_estimators`, and the depth of the trees, `max_depth`. Initialize an instance of `GridSearchCV` using the grid you defined. Set `scoring` to `\"accuracy\"` and the number of folds to 5 - which is done using the `cv` argument.\n",
    "As we have seen earlier a grid can be defined in the following way:\n",
    "```python\n",
    "par_grid = {'A': [1,2,3], 'B': [10,100,150]}\n",
    "```\n",
    "Be aware that the size of the grid grows exponentially in the number of values!\n",
    "The `GridSearchCV` takes an estimator as argument. An estimator is a model object such as `RandomForestClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "par_grid = {'n_estimators': [10, 100, 150], 'max_depth': [1, 3, 5]}\n",
    "\n",
    "# Initialize model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Initialize cross validation grid search\n",
    "gcv = GridSearchCV(clf, par_grid, scoring = \"accuracy\", cv = 5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the initialized `GridSearchCV` object on the `X_train` and `y_train` data from the titanic dataset. Identify the best set of parameters, which is done with the `best_params_` attribute. The scores for all classifiers can be found with \n",
    "```python\n",
    "pd.DataFrame(fit_res.cv_results_)\n",
    "```\n",
    "Where `fit_res` is the object returned by the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "\n",
    "# Fit model\n",
    "gcv.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparam\n",
    "print(gcv.best_params_)\n",
    "\n",
    "# Show table of scores\n",
    "pd.DataFrame(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the best model on the test set. Correct the cell below and run it, in order to create a test dataset. Notice that a method in the impute section should be replaced - What should it be replaced with? Can we ensure that no information is leaked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "test0 = pd.read_csv('../../day_1/exercises/titanic_test_new.csv', index_col = 0)\n",
    "\n",
    "# Separate labels\n",
    "y_test = test0['Survived']\n",
    "X_test0 = test0.drop('Survived', axis=1)\n",
    "\n",
    "# Columns to perform one hot encoding on\n",
    "ohe_cols = ['Sex', 'Embarked']\n",
    "\n",
    "# String, index and label columns to drop\n",
    "drop_cols = ['Cabin', 'Ticket', 'Name', 'PassengerId']\n",
    "\n",
    "# Create OHE features\n",
    "test_ohe = pd.get_dummies(X_test0, columns = ohe_cols)\n",
    "\n",
    "# Drop string cols\n",
    "test_drop = test_ohe.drop(drop_cols, axis = 1)\n",
    "\n",
    "# Impute missing values\n",
    "X_test = pd.DataFrame(\n",
    "    imputer.SOME_METHOD(X = test_drop), #### REPLACE SOME_METHOD with a real method ####\n",
    "    columns = test_drop.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "test0 = pd.read_csv('../../day_1/exercises/titanic_test_new.csv', index_col = 0)\n",
    "\n",
    "# Separate labels\n",
    "y_test = test0['Survived']\n",
    "X_test0 = test0.drop('Survived', axis=1)\n",
    "\n",
    "# Columns to perform one hot encoding on\n",
    "ohe_cols = ['Sex', 'Embarked']\n",
    "\n",
    "# String, index and label columns to drop\n",
    "drop_cols = ['Cabin', 'Ticket', 'Name', 'PassengerId']\n",
    "\n",
    "# Create OHE features\n",
    "test_ohe = pd.get_dummies(X_test0, columns = ohe_cols)\n",
    "\n",
    "# Drop string cols\n",
    "test_drop = test_ohe.drop(drop_cols, axis = 1)\n",
    "\n",
    "# Impute missing values\n",
    "X_test = pd.DataFrame(\n",
    "    imputer.transform(X = test_drop), # The imputer is a transformer, thus we use transform\n",
    "    columns = test_drop.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `GridSearchCV` object that you trained earlier, to predict on the test data. Calculate the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "\n",
    "# Predict\n",
    "pred = gcv.predict(X_test)\n",
    "\n",
    "# Evaluate score\n",
    "print(\"Test accuracy is: {}\".format(np.sum(pred == y_test)/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "We will now do a similar exercise using XGBoost. Make sure to execute the cells above that generate the training and testing datasets, as these will be reused.\n",
    "Import the `XGBClassifier` from `xgboost` and initialize the model. Create a parameter grid for the variables `max_depth`, `learning_rate` and `n_estimators`. As before don't use too many values!\n",
    "Then do a grid search using `GridSearchCV` again. Make sure to set the parameter `return_train_score=True`\n",
    "\n",
    "\n",
    "You can checkout all the parameters using this link: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "\n",
    "NB: If you are getting a lot of warnings when running the GridSearchCV with XGBoost, try adding `'eval_metric': [\"logloss\"]` to the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize model\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "par_grid = {'n_estimators': [10, 30, 50], 'max_depth': [1, 3], 'learning_rate': [0.01, 0.1]}\n",
    "\n",
    "# Initialize cross validation grid search\n",
    "gcv_b = GridSearchCV(xgb, par_grid, scoring = \"accuracy\", cv = 5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on the training data and extract the score and best parameters. Checkout the `mean_train_score` and `mean_test_score`, how do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "\n",
    "# Fit model\n",
    "gcv_b.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparam\n",
    "print(gcv_b.best_params_)\n",
    "\n",
    "# Show table of scores\n",
    "pd.DataFrame(gcv_b.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and change you parameter grid and see if you can produce a better score. Afterwards, evaluate the best model on the test set and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "\n",
    "# Predict\n",
    "pred = gcv_b.predict(X_test)\n",
    "\n",
    "# Evaluate score\n",
    "print(\"Test accuracy is: {}\".format(np.sum(pred == y_test)/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance XGBoost\n",
    "Complex models such as Random Forest and Boosting are hard to interpret as opposed to single classification trees. This is due to the sheer number of trees and in the case of boosting the interaction between the trees. In order to get an idea of, which variables are deemed important by the model, we can try to plot the feature importance.\n",
    "\n",
    "We will use the boosting model. If you didn't complete the boosting exercise, run the answer cell. The first step is to extract the best boosting model found above. This is done using the `best_estimator_` attribute on the `GridSearchCV` object. Name the extracted object `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "model = gcv_b.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the built-in function `plot_importance` which can be imported from the `xgboost` module. The function takes the model as the first argument. Do a lookup in the documentation (Shift+Tab) and figure out how to plot both `weight` and `gain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a high resolution axis object to plot on\n",
    "f, ax = plt.subplots(dpi = 200)\n",
    "\n",
    "# Plot weight on axis object\n",
    "plot_importance(model, importance_type = 'weight', ax = ax)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots(dpi = 200)\n",
    "plot_importance(model, importance_type = 'gain', ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are only some of the features shown in all the plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance for Random Forest\n",
    "In the Random Forest implementation in `sklearn` it is only possible to extract the gain importance. As before,  the actual model object can be extracted using the `best_estimator_` attribute on the `GridSearchCV` object. Extract this and make a plot as before, by using the function below. Notice that it returns two values. \n",
    "\n",
    "A function that returns two values is used like this:\n",
    "```python\n",
    "def ret_2_vals(x):\n",
    "    return x, x + 1\n",
    "\n",
    "val0, val1 = ret_2_vals(1)\n",
    "\n",
    "print(val0)\n",
    ">>> 1\n",
    "print(val1)\n",
    ">>> 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_extract_feat_imp(model, column_names):\n",
    "    '''\n",
    "    Function to extract feature importance from a Random Forest model object.\n",
    "    \n",
    "    model: A fitted RandomForest model.\n",
    "    column_names: The column names of the input dataframe used to fit the model.\n",
    "                                  Can be extracted from the dataframe as `df.columns`.\n",
    "    \n",
    "    Returns an array with feature names and an array with the corresponding feature\n",
    "    importance scores.\n",
    "    '''\n",
    "    # Get feature importances\n",
    "    score = model.feature_importances_\n",
    "    \n",
    "    # Sort feature names according to score\n",
    "    score_dict = sorted(zip(score, column_names), reverse = True)\n",
    "    \n",
    "    # Extract sorted scores and feature names in separate lists\n",
    "    scores, feature_names = zip(*score_dict)\n",
    "    \n",
    "    return np.array(feature_names), np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract model\n",
    "model_rf = gcv.best_estimator_\n",
    "\n",
    "# Get feature names and scores\n",
    "feature_names, scores = rf_extract_feat_imp(model_rf, X_train.columns)\n",
    "\n",
    "# Plot\n",
    "plt.figure(dpi=200)\n",
    "sns.barplot(scores, feature_names)\n",
    "plt.title('Random Forest - Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For boosting the built in function determines how to plot the feature importance. However, if we want to plot them ourselves, or use the values for other purposes, we will have to extract them manually. The feature importances can be extracted in the following way:\n",
    "```python\n",
    "score_dict = model.get_booster().get_score(importance_type = \"FEAT_IMP_TYPE\")\n",
    "```\n",
    "where `FEAT_IMP_TYPE` can be either `weight` or `gain`. Create a bar plot of each of these.\n",
    "\n",
    "Otherwise you can use the function below to extract the scores. Notice that it returns two values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_extract_feat_imp(model, importance_type):\n",
    "    '''\n",
    "    Function to extract feature importance from xgboost model object.\n",
    "    \n",
    "    model: A fitted XGBoost model.\n",
    "    importance_type: The of feature importance to extract, should be either weight or gain,\n",
    "    \n",
    "    Returns an array with feature names and an array with the corresponding feature\n",
    "    importance scores.\n",
    "    '''\n",
    "    \n",
    "    score_dict = model.get_booster().get_score(importance_type = importance_type)\n",
    "    \n",
    "    # Sort feature names according to score\n",
    "    feature_names = sorted(score_dict, key = score_dict.__getitem__, reverse = True)\n",
    "    \n",
    "    # Create a sorted list of scores\n",
    "    scores = [score_dict[z] for z in feature_names]\n",
    "    \n",
    "    return np.array(feature_names), np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Solution\n",
    "\n",
    "# List of different feature importance types\n",
    "feat_imp_types = ['gain', 'weight']\n",
    "\n",
    "# Create a plot for each type\n",
    "for imp_type in feat_imp_types:\n",
    "    \n",
    "    # Use the function to extract feature importances\n",
    "    feature_names, scores = xgboost_extract_feat_imp(model, imp_type)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(dpi=200)\n",
    "    sns.barplot(scores, feature_names)\n",
    "    plt.title(imp_type)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG\n",
    "# Hide code tagged with #ANS\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "function code_hide() {\n",
    "    var cells = IPython.notebook.get_cells()\n",
    "    cells.forEach(function(x){ if(x.get_text().includes(\"#ANS\")){\n",
    "        if (x.get_text().includes(\"#CONFIG\")){\n",
    "\n",
    "        } else{\n",
    "            x.input.hide()\n",
    "            x.output_area.clear_output()\n",
    "        }\n",
    "\n",
    "        \n",
    "    }\n",
    "    })\n",
    "}\n",
    "function code_hide2() {\n",
    "    var cells = IPython.notebook.get_cells();\n",
    "    cells.forEach(function(x){\n",
    "    if( x.cell_type != \"markdown\"){\n",
    "        x.input.show()      \n",
    "    }\n",
    "    \n",
    "        });\n",
    "} \n",
    "$( document ).ready(code_hide);\n",
    "$( document ).ready(code_hide2);\n",
    "</script>\n",
    "<form action=\"javascript:code_hide()\"><input type=\"submit\" value=\"Hide answers\"></form>\n",
    "<form action=\"javascript:code_hide2()\"><input type=\"submit\" value=\"Show answers\"></form>''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
